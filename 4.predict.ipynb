{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame the problem\n",
    "\n",
    "**Challenge** : Predict the consumption of electrictity in Paris every 15 minutes at J+1\n",
    "\n",
    "**Goal** : implement a model to estimate the electrictity consumption based on some historical data. It is a typical supervised learning task since we will be working with some labelled training examples (each instance comes with the expected output, ie. the electricity consumption in Paris for a given date). Moreover, it is also a typical regression task, since we try to predict a value. More specifically this is a multivariate regression problem since the system will use multiple features to make a prediction.\n",
    "\n",
    "# Select a performance measure\n",
    "\n",
    "Accuracy of the models will be measured with the **mean absolute percentage error (MAPE)**.  \n",
    "It is basically a measurement of prediction accuracy.\n",
    "\n",
    "$ M = \\frac{100\\%}{n} \\sum_{i=1}^n | \\frac{C_i - {C_i}^*}{C_i} |\\ $\n",
    "\n",
    "\n",
    "where $C_i$  is the real consumption, $C_i^âˆ—$ the estimated consumption, and n the number of guess (96 for one day).\n",
    "\n",
    "\n",
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import requests\n",
    "import bs4 as BeautifulSoup\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/after_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a test set\n",
    "\n",
    "A test set must be set aside as soon as possible to avoid overfitting.   \n",
    "Creating a test set is theoretically quite simple: just pick some instances randomly,\n",
    "typically 20% of the dataset, set them aside and you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"Conso\",\"Date\"], axis=1).copy()\n",
    "y = df[\"Conso\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : random sampling is good if the dataset is large enough relative to the number of attributes, when it is not your risk of introducing a sampling bias and should prefere a stratified sampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "Most of machine learning algorithm are based on distances (for example the euclidian distance).   \n",
    "When features have very different scales this can cause issues, the distances are dominated by some columns ....  \n",
    "There are two types of feature scaling : **Standardisation** and **Normalisation**. \n",
    "\n",
    "Feature scaling can also help algorithm to converge faster. \n",
    "\n",
    "\n",
    "### Standardisation\n",
    "This one does not bound values to a range but is less affected by outliers (which would crush values in normalisation...)\n",
    "\n",
    "$$X_{stand} = \\frac{x - mean(x)}{std(x)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling text attributes\n",
    "Most machine learning algorithm prefer to work with numerical values so you will need to convert text labels to numbers.\n",
    "\n",
    "\n",
    "### OneHotEncoding\n",
    "If you have ordinal categories (with a notion of order, like for example the size of a t-shirt), then transforming text categories to 1,2,3 ... is good, as long as the number attributed are in the same order.  \n",
    "But if you have nominal attributes, this notion of order will bias your model, so you need to make further transformations to your dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building pipelines \n",
    "\n",
    "Scikit learn pipelines allow us to perform many transformations at once and save time.   \n",
    "Each transformer output is sent as an input of the next transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "num_attribs = ['is_weekend','Temp','lag_7_days','lag_1_day','heating_degrees','cooling_degrees','consumption_rolling_3_days']\n",
    "cat_attribs = ['weekday','month','year']\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(cat_attribs)),\n",
    "    ('one_hot_encoder', OneHotEncoder()),\n",
    "])\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepared = full_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a LinearRegression model\n",
    "Let's first train a simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = X_train.iloc[:1]\n",
    "some_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the full pipeline on a few training instances\n",
    "some_data = X_train.iloc[:5]\n",
    "some_labels = y_train.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption_predictions = lin_reg.predict(X_train_prepared)\n",
    "mean_absolute_percentage_error(y_train,consumption_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_train_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption_predictions = tree_reg.predict(X_train_prepared)\n",
    "mean_absolute_percentage_error(y_train,consumption_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's strange, we may be overfitting the data here\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data.\n",
    "\n",
    "## Better evaluation using cross validation\n",
    "\n",
    "The following code performs K-fold cross-validation:   \n",
    "It randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the decision tree model 10 times,\n",
    "picking a different fold for evaluation every time and training on the other 9 folds.\n",
    "The result is an array containing the 10 evaluation scores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(tree_reg, X_train_prepared, y_train,scoring=\"neg_mean_absolute_error\", cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing an algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "\n",
    "regressions = {}\n",
    "\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "for regressor in [ElasticNet,DecisionTreeRegressor, GradientBoostingRegressor,RandomForestRegressor,Ridge, Lasso, LinearRegression]:\n",
    "    reg = regressor()\n",
    "    regressions[reg.__class__.__name__] = reg\n",
    "    print(reg.__class__.__name__)\n",
    "    print(cross_val_score(reg, X_train_prepared[:10000], y_train[:10000],scoring=\"neg_mean_absolute_error\", cv=5).mean()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_estimators':(10,100,1000), 'max_depth':(1,10,100)}\n",
    "\n",
    "random_forest = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(random_forest, parameters)\n",
    "grid_search.fit(X_train_prepared[:2000], y_train[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a prediction for tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"Conso\",\"Date\"], axis=1)\n",
    "y = df[\"Conso\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prepared = full_pipeline.fit_transform(X)\n",
    "rf = RandomForestRegressor(max_depth=100)\n",
    "rf.fit(X_prepared,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour9 = {\n",
    "    \"Temp\":13,\n",
    "    \"weekday\":3,\n",
    "    \"month\":10,\n",
    "    \"year\":2018,\n",
    "    \"hour\":9,\n",
    "    \"lag_7_days\":8058,\n",
    "    \"lag_1_day\":8195,\n",
    "    \"is_day_off\":0,\n",
    "    \"heating_degrees\":5,\n",
    "    \"cooling_degrees\":0,\n",
    "    \"is_weekend\":0,\n",
    "    \"consumption_rolling_3_days\":100000,\n",
    "}\n",
    "X = pd.DataFrame([hour9], index=[0])\n",
    "\n",
    "X_t = full_pipeline.transform(X)\n",
    "rf.predict(X_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8362.4 - 8228.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
